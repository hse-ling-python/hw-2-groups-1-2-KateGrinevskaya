{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 4. Гриневская Катя, БКЛ182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я бы очень хотела использовать и проверку моего кода на соответствие pep-8, и замерить время работы, но, к сожалению, pep-8 и %%time не работают одновременно. Я проверила свой код на pep-8 в другой тетрадке, её я тоже прикрепила в репозиторий, в которой я на малюсеньком тексте провела эту же работу. В этой тетрадке я не буду устанавливать pip, чтобы посчитать время работы моего кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pycodestyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Обработать книгу с помощью mystem:\n",
    "\n",
    "- распарсить с помощью mystem\n",
    "- замерить время работы\n",
    "- сохранить результат в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from pymystem3) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.8)\n"
     ]
    }
   ],
   "source": [
    "# устанавливаем mystem\n",
    "! pip install pymystem3\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#открываем книгу\n",
    "with open('андерсон_яйцо.txt', encoding='utf-8') as fh:\n",
    "    text = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ипортируем json \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "# считаем время и записываем результат в файл\n",
    "%time ana = m.analyze(text)\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ana, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Обработать книгу через pymorphy\n",
    "\n",
    "- токенизировать текст с помощью nltk\n",
    "- разобрать слова с помощью pymorphy\n",
    "- замерить время работы\n",
    "- сохраните результат (хотя бы лемма + часть речи) в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\user\\anaconda3\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "# устанавливаем pymorphy\n",
    "! pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# токенизируем текст\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# создаём список, который потом отправим в файл json\n",
    "f2_list = []\n",
    "# создаём переменную для кол-ва слов\n",
    "tok_num = 0\n",
    "for token in tokens:\n",
    "    # очищаем от знаков препинания\n",
    "    if token not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"~№=-:;\"\"\":\n",
    "        tok_num += 1\n",
    "        # разбираем токены с помощью pymorphy\n",
    "        anal = morph.parse(token)\n",
    "        token_dict = {}\n",
    "        token_dict['Слово'] = anal[0].word\n",
    "        token_dict['Лемма'] = anal[0].normal_form\n",
    "        token_dict['Часть речи'] = anal[0].tag.POS\n",
    "        f2_list.append(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вносим всё в новый файл\n",
    "with open('data_2.json', 'w', encoding='utf-8') as fi:\n",
    "    json.dump(f2_list, fi, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### А ещё у меня проблемы с принтом, и с обычным, и с pprint, поэтому у меня не получится сделать красивый вывод, а только самый обыкновенный (просто вывести переменную)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Ответить на вопросы:\n",
    "\n",
    "- Какую долю слов составляет каждая часть речи? (Например, для глагола - это количество глаголов, деленное на общее число слов в тексте)\n",
    "- Найдите топ-20 (по частотности) глаголов и наречий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NPRO': '8.462958080674927%',\n",
       " 'ADJS': '0.9754811494858951%',\n",
       " 'CONJ': '10.150276825731611%',\n",
       " 'PREP': '12.47034010018455%',\n",
       " 'ADJF': '10.176641181123122%',\n",
       " 'NOUN': '27.99894542578434%',\n",
       " 'VERB': '13.735829158977065%',\n",
       " 'NUMR': '0.7909306617453202%',\n",
       " 'PRTF': '1.1336672818349591%',\n",
       " 'ADVB': '6.432902715528606%',\n",
       " 'INFN': '2.7946216715001317%',\n",
       " 'PRCL': '2.610071183759557%',\n",
       " 'GRND': '0.8963880833113631%',\n",
       " None: '0.5009227524387029%',\n",
       " 'PRED': '0.3163722646981281%',\n",
       " 'PRTS': '0.3691009754811495%',\n",
       " 'COMP': '0.15818613234906406%',\n",
       " 'INTJ': '0.02636435539151068%'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# список для всех частей речи\n",
    "gr_list = []\n",
    "# словарь с парами (часть речи; частота в тексте)\n",
    "gr_freq = {}\n",
    "# заполняем список частями речи, встр. в тексте\n",
    "for token in tokens:\n",
    "    if token not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"~№=-:;\"\"\":\n",
    "        anal = morph.parse(token)\n",
    "        gr_list.append(anal[0].tag.POS)\n",
    "gr_dict = Counter(gr_list)\n",
    "# заполняем словарь с парами (часть речи; частота)\n",
    "for key in gr_dict.keys():\n",
    "    numb = int(gr_dict[key])\n",
    "    freq = str((numb/tok_num)*100) + '%'\n",
    "    gr_freq[key] = freq\n",
    "# повторяюсь, вывести могу только так\n",
    "gr_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Второй пункт отдельно выведу для глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('быть', 65),\n",
       " ('стать', 11),\n",
       " ('знать', 8),\n",
       " ('начать', 8),\n",
       " ('поставить', 8),\n",
       " ('мочь', 7),\n",
       " ('говорить', 7),\n",
       " ('сказать', 6),\n",
       " ('приходить', 6),\n",
       " ('иметь', 5),\n",
       " ('ждать', 5),\n",
       " ('думать', 5),\n",
       " ('упасть', 5),\n",
       " ('сделать', 5),\n",
       " ('читать', 4),\n",
       " ('идти', 4),\n",
       " ('видеть', 4),\n",
       " ('хотеть', 4),\n",
       " ('находиться', 4),\n",
       " ('бывать', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список для глаголов\n",
    "v_list = []\n",
    "# список для наречий\n",
    "a_list = []\n",
    "for token in tokens:\n",
    "    if token not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"~№=-:;\"\"\":\n",
    "        anal = morph.parse(token)\n",
    "        if (anal[0].tag.POS == 'VERB')or(anal[0].tag.POS == 'INFN'):\n",
    "            v_list.append(anal[0].normal_form)\n",
    "        if anal[0].tag.POS == 'ADVB':\n",
    "            a_list.append(anal[0].normal_form)\n",
    "Counter(v_list).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### И отдельно для наречий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('затем', 13),\n",
       " ('снова', 9),\n",
       " ('несколько', 8),\n",
       " ('где', 7),\n",
       " ('уже', 7),\n",
       " ('много', 6),\n",
       " ('прежде', 4),\n",
       " ('всего', 4),\n",
       " ('более', 4),\n",
       " ('там', 4),\n",
       " ('теперь', 4),\n",
       " ('обратно', 4),\n",
       " ('потом', 3),\n",
       " ('несомненно', 3),\n",
       " ('возле', 3),\n",
       " ('иногда', 3),\n",
       " ('почему', 3),\n",
       " ('редко', 3),\n",
       " ('вечером', 3),\n",
       " ('здесь', 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a_list).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Посмотрите документацию для nltk н-грамм (nltk.bigrams, например), попробуйте составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания). Почему получаются именно такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### отдельно для биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('что', 'он'), 10),\n",
       " (('он', 'быть'), 8),\n",
       " (('о', 'тот'), 8),\n",
       " (('с', 'матерь'), 8),\n",
       " (('в', 'город'), 7),\n",
       " (('тот', 'что'), 7),\n",
       " (('яйцо', 'и'), 7),\n",
       " (('джо', 'кейн'), 7),\n",
       " (('в', 'это'), 6),\n",
       " (('в', 'тот'), 6),\n",
       " (('а', 'затем'), 6),\n",
       " (('он', 'в'), 6),\n",
       " (('на', 'прилавок'), 6),\n",
       " (('и', 'с'), 5),\n",
       " (('у', 'он'), 5),\n",
       " (('на', 'свет'), 5),\n",
       " (('и', 'я'), 5),\n",
       " (('что', 'я'), 5),\n",
       " (('поставить', 'яйцо'), 5),\n",
       " (('уверенный', 'что'), 4),\n",
       " (('что', 'у'), 4),\n",
       " (('на', 'ферма'), 4),\n",
       " (('город', 'бидуэла'), 4),\n",
       " (('прежде', 'всего'), 4),\n",
       " (('что', 'в'), 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пустой список для лемматиз. текста\n",
    "lemmas_text = []\n",
    "for token in tokens:\n",
    "    if token not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"~№=-:;\"\"\":\n",
    "        anal = morph.parse(token)\n",
    "        lemmas_text.append(anal[0].normal_form)\n",
    "# список биграм лемматиз. текста\n",
    "lemmas_bi = list(nltk.bigrams(lemmas_text))\n",
    "Counter(lemmas_bi).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### и отдельно для триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('мы', 'с', 'матерь'), 3),\n",
       " (('быть', 'уверенный', 'что'), 3),\n",
       " (('что', 'он', 'хотеть'), 3),\n",
       " (('у', 'он', 'быть'), 3),\n",
       " (('поставить', 'яйцо', 'стоймя'), 3),\n",
       " (('город', 'бидуэла', 'штат'), 2),\n",
       " (('бидуэла', 'штат', 'огайо'), 2),\n",
       " (('ферма', 'в', 'город'), 2),\n",
       " (('в', 'бар', 'бена'), 2),\n",
       " (('бар', 'бена', 'хеда'), 2),\n",
       " (('в', 'десять', 'часы'), 2),\n",
       " (('в', 'тот', 'время'), 2),\n",
       " (('о', 'тот', 'чтобы'), 2),\n",
       " (('в', 'это', 'мир'), 2),\n",
       " (('читать', 'книга', 'и'), 2),\n",
       " (('о', 'тот', 'как'), 2),\n",
       " (('появление', 'на', 'свет'), 2),\n",
       " (('о', 'тот', 'что'), 2),\n",
       " (('с', 'самый', 'начало'), 2),\n",
       " (('в', 'пот', 'лицо'), 2),\n",
       " (('к', 'свой', 'создатель'), 2),\n",
       " (('отец', 'с', 'матерь'), 2),\n",
       " (('в', 'город', 'бидуэла'), 2),\n",
       " (('заняться', 'ресторанный', 'дело'), 2),\n",
       " (('а', 'затем', 'в'), 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_tri = list(nltk.trigrams(lemmas_text))\n",
    "Counter(lemmas_tri).most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
